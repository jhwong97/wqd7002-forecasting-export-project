{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15e7d2fa-c174-4c6a-9dd6-62f509770eef",
   "metadata": {},
   "source": [
    "# Forecasting Malaysia's Total Export Value with Machine Learning Regression Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b3f50a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Import libraries and packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cbdeb370-2c23-488d-8683-be3930d20dc5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import logging\n",
    "from airflow.exceptions import AirflowFailException\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Tag\n",
    "from fredapi import Fred\n",
    "from imfpy import searches, retrievals\n",
    "from typing import Optional\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util import Retry\n",
    "from dotenv import load_dotenv\n",
    "from google.cloud import storage\n",
    "from google.cloud import bigquery\n",
    "from google.cloud.exceptions import NotFound\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from pmdarima.arima import auto_arima\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn import metrics #to evaluate the accuracy of our ml model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from pmdarima.arima import auto_arima\n",
    "from lazypredict.Supervised import LazyRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e84125e-fc56-4a82-9989-dd33e9ae7019",
   "metadata": {
    "tags": []
   },
   "source": [
    "## ETL Process\n",
    "### Malaysia's Total Export and Import Value from METS OnlineFred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f3267d-6a8f-405e-b87c-117568aeb24e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Functions for Google Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09205985-5d83-4300-a34b-6a003095d560",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a function for uploading data to Google Storage Bucket    \n",
    "def upload_to_bucket(storage_client,\n",
    "                     bucket_name,\n",
    "                     df_list):\n",
    "    if df_list is None:\n",
    "        logging.warning(\"Please ensure df_list has a value.\")\n",
    "    \n",
    "    my_bucket = storage_client.bucket(bucket_name)\n",
    "    # Check if the specified bucet_name exists or not\n",
    "    if not my_bucket.exists(): # If the bucket does not exist\n",
    "        try:\n",
    "            logging.info(f'Bucket - {bucket_name} is not found.')\n",
    "            logging.info(f\"Creating {bucket_name} in progress ...\")\n",
    "            my_bucket.create() # Create the bucket with the specified name\n",
    "            logging.info(f\"SUCCESS: {bucket_name} has been created.\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.info(f\"Error creating bucket: {e}\")\n",
    "                  \n",
    "    else:\n",
    "        logging.info(f\"Bucket - {bucket_name} is found.\")\n",
    "   \n",
    "    try:\n",
    "        # To create a list to store the gsutil uri \n",
    "        gsutil_uri_list = []\n",
    "        for item in df_list:\n",
    "            # timestamp = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S ')\n",
    "            # blob_name = item.name + \" \" + timestamp\n",
    "            blob_name = item.name\n",
    "            blob = my_bucket.blob(blob_name)\n",
    "            logging.info(f\"Uploading data to Google Storage Bucket in progress ...\")\n",
    "            blob.upload_from_string(item.to_csv(index=False), 'text/csv')\n",
    "            logging.info(f'SUCCESS: {blob} has successfully uploaded to {my_bucket}.')\n",
    "            gsutil_uri = f\"gs://{bucket_name}/{blob_name}\"\n",
    "            gsutil_uri_list.append(gsutil_uri) \n",
    "        return gsutil_uri_list\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error: {e}\")\n",
    "\n",
    "# Define a function to load data from google bucket to google bigquery\n",
    "def upload_to_bigquery(client, dataset_name, table_name, job_config, gsutil_uri):\n",
    "    try:\n",
    "        for i in range(len(table_name)): \n",
    "            # Look up for dataset\n",
    "            dataset_ref = client.dataset(dataset_name)\n",
    "            try:\n",
    "                targeted_dataset = client.get_dataset(dataset_ref)\n",
    "                logging.info(f'Dataset {targeted_dataset.dataset_id} already exists.')\n",
    "            # Create new dataset if not found\n",
    "            except NotFound:\n",
    "                logging.info(f\"Dataset {dataset_ref} is not found\")\n",
    "                logging.info(f\"Creating dataset - {dataset_ref} in progress ...\")\n",
    "                targeted_dataset = client.create_dataset(dataset_ref)\n",
    "                logging.info(f'Dataset {targeted_dataset.dataset_id} created.')\n",
    "\n",
    "            # Look up for table\n",
    "            table_ref = dataset_ref.table(table_name[i])\n",
    "            try:\n",
    "                targeted_table = client.get_table(table_ref)\n",
    "                logging.info(f'Table {targeted_table.table_id} already exists.')\n",
    "            # Create new table if not found\n",
    "            except NotFound:\n",
    "                logging.info(f\"Dataset {table_ref} is not found\")\n",
    "                logging.info(f\"Creating table - {table_ref} in progress ...\")\n",
    "                targeted_table = client.create_table(table_ref)\n",
    "                logging.info(f'Table {targeted_table.table_id} created.')\n",
    "\n",
    "            # Upload the data to bigquery table using gsutil URI\n",
    "            load_job = client.load_table_from_uri(gsutil_uri[i],\n",
    "                                                  targeted_table, \n",
    "                                                  job_config=job_config)\n",
    "\n",
    "            logging.info(load_job.result())\n",
    "            logging.info(f\"SUCCESS: The data has been loaded to Google BigQuery.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error: {e}\")\n",
    "        \n",
    "def create_blob_list(storage_client, \n",
    "                     bucket_name, \n",
    "                     df_list):\n",
    "    if df_list is None:\n",
    "        logging.warning(\"Please ensure df_list has a value.\")\n",
    "        exit()\n",
    "    \n",
    "    my_bucket = storage_client.bucket(bucket_name)\n",
    "    # Check if the specified bucet_name exists or not\n",
    "    if not my_bucket.exists(): # If the bucket does not exist\n",
    "        try:\n",
    "            logging.info(f'Bucket - {bucket_name} is not found.')\n",
    "            logging.info(f\"Creating {bucket_name} in progress ...\")\n",
    "            my_bucket.create() # Create the bucket with the specified name\n",
    "            logging.info(f\"SUCCESS: {bucket_name} has been created.\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error creating bucket: {e}\")\n",
    "                  \n",
    "    else:\n",
    "        logging.info(f\"Bucket - {bucket_name} is found.\")\n",
    "    \n",
    "    try:\n",
    "        # To create a list to store all the blobs uploaded to the bucket\n",
    "        blob_list =[]\n",
    "        for item in df_list:\n",
    "            blob_name = item.name\n",
    "            blob = my_bucket.blob(blob_name)\n",
    "            logging.info(f\"Uploading data to Google Storage Bucket in progress ...\")\n",
    "            blob.upload_from_string(item.to_csv(index=False), 'text/csv')\n",
    "            logging.info(f\"SUCCESS: {blob} has been uploaded to {my_bucket}.\")\n",
    "            blob_list.append(blob_name)\n",
    "        return blob_list\n",
    "            \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e543042-b0ac-4b20-b640-065b77a372c8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Functions for METS ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40f85583-ba0a-472a-b50f-93699018477b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a function for extracting raw data from METS Online\n",
    "def mets_extract(url,\n",
    "                 payload: Optional[dict] = None,\n",
    "                 headers: Optional[dict] = None):\n",
    "    \n",
    "    # Specify the maximum number of retry\n",
    "    MAX_RETRIES = 5\n",
    "    \n",
    "    # Define the retry strategy\n",
    "    retry_strategy = Retry(total = MAX_RETRIES,\n",
    "                           backoff_factor = 2,\n",
    "                           status_forcelist = [429, 500, 502, 503, 504],\n",
    "                           allowed_methods = [\"GET\", \"POST\"])\n",
    "    \n",
    "    # Create an HTTP adapter with the retry strategy and mount it to session\n",
    "    adapter = HTTPAdapter(max_retries=retry_strategy)\n",
    "    \n",
    "    # Create a new session object\n",
    "    session = requests.Session()\n",
    "    session.mount('http://', adapter)\n",
    "    session.mount('https://', adapter)\n",
    "    \n",
    "    logging.info(\"Extracting raw data in progress ...\")\n",
    "    # Make a request using the created session object\n",
    "    raw_data = session.post(url,\n",
    "                            data = payload,\n",
    "                            headers = headers,\n",
    "                            verify = False)\n",
    "    \n",
    "    if raw_data.status_code == 200:\n",
    "        logging.info('SUCCESS: Raw Data has been extracted.')\n",
    "\n",
    "        # Parse the HTML\n",
    "        raw_data = BeautifulSoup(raw_data.text, 'html.parser')\n",
    "        return raw_data\n",
    "\n",
    "    else:\n",
    "        logging.error(f'Error: Fail to extract the raw data. ErrorCode: {raw_data.status_code}.')\n",
    "\n",
    "# Define a function for basic preprocessing on the extracted raw html text.\n",
    "def mets_preprocess(raw_data, \n",
    "                    dataframe_name):\n",
    "    \n",
    "    # Look up for the table\n",
    "    result = raw_data.find('table', class_='table-bordered')\n",
    "    \n",
    "    # Extract table rows\n",
    "    rows = result.find_all('tr')\n",
    "    \n",
    "    individual_data = []\n",
    "    for row in rows:\n",
    "        data = row.find_all(['th', 'td'])\n",
    "        if data:\n",
    "            data = [item.get_text(strip=True) for item in data]\n",
    "            if 'GRAND TOTAL' not in data:\n",
    "                individual_data.append(data)\n",
    "    \n",
    "    logging.info('Converting raw data to dataframe......')\n",
    "    # Select a subset of columns from the first row as column names\n",
    "    df = pd.DataFrame(individual_data[1:], columns=individual_data[0])\n",
    "    # To remove the yearly data column due to redundancy \n",
    "    data_month_filter = [col for col in df.columns if '-' not in col or not any(char.isdigit() for char in col)]\n",
    "    df_list = []\n",
    "    df_monthly = df.loc[:, data_month_filter]\n",
    "    df_monthly.name = dataframe_name\n",
    "    df_list.append(df_monthly)\n",
    "    logging.info(f'SUCCESS: {dataframe_name} has been created')\n",
    "    \n",
    "    return df_list\n",
    "\n",
    "def mets_transformation(df_list, \n",
    "                        new_column_name, \n",
    "                        dataframe_name):\n",
    "    \n",
    "    transformed_df_list = []\n",
    "    replacements = {' ': '_',\n",
    "                    '&': '',\n",
    "                    ',': '',\n",
    "                    '.': '',}\n",
    "    try:\n",
    "        for item in df_list:\n",
    "            df = item.transpose().reset_index()\n",
    "            column_name = list(df.iloc[2,:])\n",
    "            edited_column_name = []\n",
    "\n",
    "            # To replace the selected symbols and empty spaces\n",
    "            for column in column_name:\n",
    "                for old, new in replacements.items():\n",
    "                    column = column.replace(old, new)\n",
    "                edited_column_name.append(column)\n",
    "\n",
    "            df = df.iloc[3:]\n",
    "            df.columns = edited_column_name\n",
    "            df = df.rename(columns={'PRODUCT_DESCRIPTION': 'date'})\n",
    "\n",
    "            # To convert the 'date' column to datetime format, whereas the others are converted to numeric.\n",
    "            for column in df.columns:\n",
    "                if column == 'date':\n",
    "                    df[column] = pd.to_datetime(df[column])\n",
    "                else:\n",
    "                    df[column] = pd.to_numeric(df[column].str.replace(',', ''))\n",
    "\n",
    "            # Create a new column which sum all the values from other columns in the same row\n",
    "            df[new_column_name] = df.iloc[:,1:].sum(axis=1)\n",
    "            df.name = dataframe_name\n",
    "\n",
    "            transformed_df_list.append(df)\n",
    "            logging.info(f\"SUCCESS: Dataframe - {dataframe_name} has been transformed.\")\n",
    "            return transformed_df_list\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81593806-16bd-40b7-b542-a8a20ae49378",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Input parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2eda8ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "# Retrieve the csrf_token and Cookie values\n",
    "csrf_token = os.getenv(\"csrf_token\")\n",
    "Cookie = os.getenv(\"cookie\")\n",
    "\n",
    "# URL of targeted site\n",
    "url = \"https://metsonline.dosm.gov.my/tradev2/product-coderesult\"\n",
    "\n",
    "# headers of targeted site\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/x-www-form-urlencoded; charset=UTF-8\",\n",
    "    \"Cookie\": \"PHPSESSID=938e8fjm1hj9bb07oq3u4fisi4; _csrf=615b79fbeb5a5e5147fee3ac2245a8abb4f3d106b51ba953154dfe4b8036f358a%3A2%3A%7Bi%3A0%3Bs%3A5%3A%22_csrf%22%3Bi%3A1%3Bs%3A32%3A%22V4KGkyVFWmBgtSAZPHpnC8MV4N1-eYuB%22%3B%7D; cookiesession1=678B288423AE9070F6271BFC1DD8DA77\"\n",
    "}\n",
    "\n",
    "# payload of targeted site\n",
    "payload_exports = {\n",
    "    \"_csrf\": \"Q3RsVE1CS1QVQCcTJjsdEhQZLjM5EQoOEzwcOg56BgJ3Ol15KBs.Fg==\",\n",
    "    \"Tradev2[typeofsearch]\": \"classification\",\n",
    "    \"Tradev2[typedigit]\": 7,\n",
    "    \"Tradev2[rangecode1]\": 0,\n",
    "    \"Tradev2[rangecode2]\": 9,\n",
    "    # 'Tradev2[code_idcode]': ,\n",
    "    # 'Tradev2[code_idcodedigit9]': ,\n",
    "    # 'Tradev2[tradeflow]': ,\n",
    "    \"Tradev2[tradeflow][]\": \"exports\",\n",
    "    # 'Tradev2[timeframe]': ,\n",
    "    \"Tradev2[timeframe]\": \"month\",\n",
    "    # 'Tradev2[rangeyear]': ,\n",
    "    # 'Tradev2[rangeyear2]': ,\n",
    "    # 'Tradev2[rangeyearone]': ,\n",
    "    # 'Tradev2[rangemonthone]': ,\n",
    "    \"Tradev2[mothdata]\": 2000,\n",
    "    \"Tradev2[mothdata2]\": 2001,\n",
    "    # 'Tradev2[classification_serch]': ,\n",
    "    # 'Tradev2[country2]': ,\n",
    "    \"Tradev2[geogroup]\": 1,\n",
    "    \"Tradev2[geogroup]\": 29,\n",
    "    \"Tradev2[codeshowby]\": \"code\",\n",
    "}\n",
    "\n",
    "# payload of targeted site\n",
    "payload_imports = {\n",
    "    \"_csrf\": \"Q3RsVE1CS1QVQCcTJjsdEhQZLjM5EQoOEzwcOg56BgJ3Ol15KBs.Fg==\",\n",
    "    \"Tradev2[typeofsearch]\": \"classification\",\n",
    "    \"Tradev2[typedigit]\": 7,\n",
    "    \"Tradev2[rangecode1]\": 0,\n",
    "    \"Tradev2[rangecode2]\": 9,\n",
    "    # 'Tradev2[code_idcode]': ,\n",
    "    # 'Tradev2[code_idcodedigit9]': ,\n",
    "    # 'Tradev2[tradeflow]': ,\n",
    "    \"Tradev2[tradeflow][]\": \"imports\",\n",
    "    # 'Tradev2[timeframe]': ,\n",
    "    \"Tradev2[timeframe]\": \"month\",\n",
    "    # 'Tradev2[rangeyear]': ,\n",
    "    # 'Tradev2[rangeyear2]': ,\n",
    "    # 'Tradev2[rangeyearone]': ,\n",
    "    # 'Tradev2[rangemonthone]': ,\n",
    "    \"Tradev2[mothdata]\": 2000,\n",
    "    \"Tradev2[mothdata2]\": 2001,\n",
    "    # 'Tradev2[classification_serch]': ,\n",
    "    # 'Tradev2[country2]': ,\n",
    "    \"Tradev2[geogroup]\": 1,\n",
    "    \"Tradev2[geogroup]\": 29,\n",
    "    \"Tradev2[codeshowby]\": \"code\",\n",
    "}\n",
    "# Google Cloud Parts\n",
    "# Convert the credentials to .json file for the usage of GOOGLE_APPLICATION_CREDENTIALS\n",
    "CREDENTIALS = json.loads(os.getenv('CREDENTIALS'))\n",
    "\n",
    "# Check if there's an existing credentials file\n",
    "with open('credentials.json', 'w') as cred_file:\n",
    "    json.dump(CREDENTIALS, cred_file)\n",
    "    \n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] ='credentials.json'\n",
    "\n",
    "storage_client = storage.Client()\n",
    "bucket_name = 'trial-malaysia-international-trade'\n",
    "bq_client = bigquery.Client()\n",
    "dataset_name = \"trial_area\"\n",
    "table_name_export = [\"trial_export_my\"]\n",
    "table_name_import = [\"trial_import_my\"]\n",
    "job_config = bigquery.LoadJobConfig(source_format=bigquery.SourceFormat.CSV,\n",
    "                                    write_disposition='WRITE_TRUNCATE',\n",
    "                                    skip_leading_rows=1,\n",
    "                                    autodetect=True,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1875edb5-cb34-486c-9ffc-6a64fac5ac3b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Test Run for Exports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e58812f7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\u001b[34m2024-02-06T09:41:00.688+0800\u001b[0m] {\u001b[34m1547133257.py:\u001b[0m23} INFO\u001b[0m - Extracting raw data in progress ...\u001b[0m\n",
      "[\u001b[34m2024-02-06T09:41:25.664+0800\u001b[0m] {\u001b[34m1547133257.py:\u001b[0m31} INFO\u001b[0m - SUCCESS: Raw Data has been extracted.\u001b[0m\n",
      "[\u001b[34m2024-02-06T09:41:25.711+0800\u001b[0m] {\u001b[34m1547133257.py:\u001b[0m58} INFO\u001b[0m - Converting raw data to dataframe......\u001b[0m\n",
      "[\u001b[34m2024-02-06T09:41:25.714+0800\u001b[0m] {\u001b[34m1547133257.py:\u001b[0m67} INFO\u001b[0m - SUCCESS: export_trial has been created\u001b[0m\n",
      "[\u001b[34m2024-02-06T09:41:25.724+0800\u001b[0m] {\u001b[34m1547133257.py:\u001b[0m108} INFO\u001b[0m - SUCCESS: Dataframe - total_export has been transformed.\u001b[0m\n",
      "[\u001b[34m2024-02-06T09:44:14.445+0800\u001b[0m] {\u001b[34m1887242903.py:\u001b[0m21} INFO\u001b[0m - Bucket - trial-malaysia-international-trade is found.\u001b[0m\n",
      "[\u001b[34m2024-02-06T09:44:14.446+0800\u001b[0m] {\u001b[34m1887242903.py:\u001b[0m31} INFO\u001b[0m - Uploading data to Google Storage Bucket in progress ...\u001b[0m\n",
      "[\u001b[34m2024-02-06T09:44:14.881+0800\u001b[0m] {\u001b[34m1887242903.py:\u001b[0m33} INFO\u001b[0m - SUCCESS: <Blob: trial-malaysia-international-trade, total_export, 1707183853058936> has successfully uploaded to <Bucket: trial-malaysia-international-trade>.\u001b[0m\n",
      "[\u001b[34m2024-02-06T09:46:00.778+0800\u001b[0m] {\u001b[34m1887242903.py:\u001b[0m49} INFO\u001b[0m - Dataset trial_area already exists.\u001b[0m\n",
      "[\u001b[34m2024-02-06T09:46:01.151+0800\u001b[0m] {\u001b[34m1887242903.py:\u001b[0m61} INFO\u001b[0m - Table trial_export_my already exists.\u001b[0m\n",
      "[\u001b[34m2024-02-06T09:46:04.233+0800\u001b[0m] {\u001b[34m1887242903.py:\u001b[0m74} INFO\u001b[0m - LoadJob<project=personal-project-401309, location=US, id=0fca46c3-7c98-4a94-bfdf-1b82ff3ef29a>\u001b[0m\n",
      "[\u001b[34m2024-02-06T09:46:04.234+0800\u001b[0m] {\u001b[34m1887242903.py:\u001b[0m75} INFO\u001b[0m - SUCCESS: The data has been loaded to Google BigQuery.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "raw_data_exports = mets_extract(url=url, payload=payload_exports, headers=headers)\n",
    "df_list = mets_preprocess(raw_data=raw_data_exports, dataframe_name='export_trial')\n",
    "transformed_df_list = mets_transformation(df_list=df_list, new_column_name=\"total_export_value\", dataframe_name=\"total_export\")\n",
    "gsutil_uri_list = upload_to_bucket(storage_client=storage_client,\n",
    "                                   bucket_name=bucket_name,\n",
    "                                   df_list=transformed_df_list)\n",
    "upload_to_bigquery(client=bq_client,\n",
    "                   dataset_name=dataset_name,\n",
    "                   table_name=table_name_export,\n",
    "                   job_config=job_config,\n",
    "                   gsutil_uri=gsutil_uri_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c07a08-a316-4a3f-93f8-ae0739990d82",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Test run for Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa21ad34-12d1-436d-b7f7-ca84caeec4ae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\u001b[34m2024-02-06T09:46:04.240+0800\u001b[0m] {\u001b[34m1547133257.py:\u001b[0m23} INFO\u001b[0m - Extracting raw data in progress ...\u001b[0m\n",
      "[\u001b[34m2024-02-06T09:46:29.556+0800\u001b[0m] {\u001b[34m1547133257.py:\u001b[0m31} INFO\u001b[0m - SUCCESS: Raw Data has been extracted.\u001b[0m\n",
      "[\u001b[34m2024-02-06T09:46:29.592+0800\u001b[0m] {\u001b[34m1547133257.py:\u001b[0m58} INFO\u001b[0m - Converting raw data to dataframe......\u001b[0m\n",
      "[\u001b[34m2024-02-06T09:46:29.594+0800\u001b[0m] {\u001b[34m1547133257.py:\u001b[0m67} INFO\u001b[0m - SUCCESS: import_trial has been created\u001b[0m\n",
      "[\u001b[34m2024-02-06T09:46:29.605+0800\u001b[0m] {\u001b[34m1547133257.py:\u001b[0m108} INFO\u001b[0m - SUCCESS: Dataframe - total_import has been transformed.\u001b[0m\n",
      "[\u001b[34m2024-02-06T09:46:29.810+0800\u001b[0m] {\u001b[34m1887242903.py:\u001b[0m21} INFO\u001b[0m - Bucket - trial-malaysia-international-trade is found.\u001b[0m\n",
      "[\u001b[34m2024-02-06T09:46:29.812+0800\u001b[0m] {\u001b[34m1887242903.py:\u001b[0m31} INFO\u001b[0m - Uploading data to Google Storage Bucket in progress ...\u001b[0m\n",
      "[\u001b[34m2024-02-06T09:46:30.246+0800\u001b[0m] {\u001b[34m1887242903.py:\u001b[0m33} INFO\u001b[0m - SUCCESS: <Blob: trial-malaysia-international-trade, total_import, 1707183988428578> has successfully uploaded to <Bucket: trial-malaysia-international-trade>.\u001b[0m\n",
      "[\u001b[34m2024-02-06T09:46:30.678+0800\u001b[0m] {\u001b[34m1887242903.py:\u001b[0m49} INFO\u001b[0m - Dataset trial_area already exists.\u001b[0m\n",
      "[\u001b[34m2024-02-06T09:46:31.077+0800\u001b[0m] {\u001b[34m1887242903.py:\u001b[0m61} INFO\u001b[0m - Table trial_import_my already exists.\u001b[0m\n",
      "[\u001b[34m2024-02-06T09:46:33.861+0800\u001b[0m] {\u001b[34m1887242903.py:\u001b[0m74} INFO\u001b[0m - LoadJob<project=personal-project-401309, location=US, id=acd88e42-aa5d-4592-9744-469999f4c538>\u001b[0m\n",
      "[\u001b[34m2024-02-06T09:46:33.862+0800\u001b[0m] {\u001b[34m1887242903.py:\u001b[0m75} INFO\u001b[0m - SUCCESS: The data has been loaded to Google BigQuery.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "raw_data_imports = mets_extract(url=url, payload=payload_imports, headers=headers)\n",
    "df_list = mets_preprocess(raw_data=raw_data_imports, dataframe_name='import_trial')\n",
    "transformed_df_list = mets_transformation(df_list=df_list, new_column_name=\"total_import_value\", dataframe_name=\"total_import\")\n",
    "gsutil_uri_list = upload_to_bucket(storage_client=storage_client,\n",
    "                                   bucket_name=bucket_name,\n",
    "                                   df_list=transformed_df_list)\n",
    "upload_to_bigquery(client=bq_client,\n",
    "                   dataset_name=dataset_name,\n",
    "                   table_name=table_name_import,\n",
    "                   job_config=job_config,\n",
    "                   gsutil_uri=gsutil_uri_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ec4d51-5e2a-489f-9001-65c7a9250e52",
   "metadata": {},
   "source": [
    "### USDMYR Exchange Rate and RBEER from Federal Reserve Economic Data (FRED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8a7242-8151-44df-8fb9-3e74e179ed6e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Functions for FRED ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45c20a49-86c5-497a-8c1b-a86a44bbd5e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fred_data_extraction(selected_data):\n",
    "    df_list = []\n",
    "    for item in selected_data:\n",
    "        logging.info(f\"Extracting {item} data in progress ...\")\n",
    "        df = fred.get_series_all_releases(item)\n",
    "        logging.info(f\"SUCCESS: {item} data has been extracted.\")\n",
    "        df.name = item\n",
    "        df_list.append(df)\n",
    "    return df_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38d5935-2490-4795-b611-f9efc2454d8c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Test Run for FRED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e624439-f642-46a8-849f-258cd51c6899",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "FRED_API = os.getenv(\"FRED_API\")\n",
    "fred = Fred(api_key = FRED_API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9f6ef04-8887-451d-b39b-ef928b8ffe14",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\u001b[34m2024-02-06T09:46:33.877+0800\u001b[0m] {\u001b[34m1846224335.py:\u001b[0m4} INFO\u001b[0m - Extracting EXMAUS data in progress ...\u001b[0m\n",
      "[\u001b[34m2024-02-06T09:46:34.571+0800\u001b[0m] {\u001b[34m1846224335.py:\u001b[0m6} INFO\u001b[0m - SUCCESS: EXMAUS data has been extracted.\u001b[0m\n",
      "[\u001b[34m2024-02-06T09:46:34.572+0800\u001b[0m] {\u001b[34m1846224335.py:\u001b[0m4} INFO\u001b[0m - Extracting RBMYBIS data in progress ...\u001b[0m\n",
      "[\u001b[34m2024-02-06T09:46:36.041+0800\u001b[0m] {\u001b[34m1846224335.py:\u001b[0m6} INFO\u001b[0m - SUCCESS: RBMYBIS data has been extracted.\u001b[0m\n",
      "[\u001b[34m2024-02-06T09:46:36.242+0800\u001b[0m] {\u001b[34m1887242903.py:\u001b[0m21} INFO\u001b[0m - Bucket - trial-malaysia-international-trade is found.\u001b[0m\n",
      "[\u001b[34m2024-02-06T09:46:36.243+0800\u001b[0m] {\u001b[34m1887242903.py:\u001b[0m31} INFO\u001b[0m - Uploading data to Google Storage Bucket in progress ...\u001b[0m\n",
      "[\u001b[34m2024-02-06T09:46:36.678+0800\u001b[0m] {\u001b[34m1887242903.py:\u001b[0m33} INFO\u001b[0m - SUCCESS: <Blob: trial-malaysia-international-trade, EXMAUS, 1707183994863265> has successfully uploaded to <Bucket: trial-malaysia-international-trade>.\u001b[0m\n",
      "[\u001b[34m2024-02-06T09:46:36.679+0800\u001b[0m] {\u001b[34m1887242903.py:\u001b[0m31} INFO\u001b[0m - Uploading data to Google Storage Bucket in progress ...\u001b[0m\n",
      "[\u001b[34m2024-02-06T09:46:37.540+0800\u001b[0m] {\u001b[34m1887242903.py:\u001b[0m33} INFO\u001b[0m - SUCCESS: <Blob: trial-malaysia-international-trade, RBMYBIS, 1707183995720820> has successfully uploaded to <Bucket: trial-malaysia-international-trade>.\u001b[0m\n",
      "[\u001b[34m2024-02-06T09:46:37.971+0800\u001b[0m] {\u001b[34m1887242903.py:\u001b[0m49} INFO\u001b[0m - Dataset trial_area already exists.\u001b[0m\n",
      "[\u001b[34m2024-02-06T09:46:38.348+0800\u001b[0m] {\u001b[34m1887242903.py:\u001b[0m61} INFO\u001b[0m - Table EXMAUS already exists.\u001b[0m\n",
      "[\u001b[34m2024-02-06T09:46:42.800+0800\u001b[0m] {\u001b[34m1887242903.py:\u001b[0m74} INFO\u001b[0m - LoadJob<project=personal-project-401309, location=US, id=571b8ce5-c28a-435c-a5c3-79f57990d478>\u001b[0m\n",
      "[\u001b[34m2024-02-06T09:46:42.801+0800\u001b[0m] {\u001b[34m1887242903.py:\u001b[0m75} INFO\u001b[0m - SUCCESS: The data has been loaded to Google BigQuery.\u001b[0m\n",
      "[\u001b[34m2024-02-06T09:46:43.201+0800\u001b[0m] {\u001b[34m1887242903.py:\u001b[0m49} INFO\u001b[0m - Dataset trial_area already exists.\u001b[0m\n",
      "[\u001b[34m2024-02-06T09:46:43.574+0800\u001b[0m] {\u001b[34m1887242903.py:\u001b[0m61} INFO\u001b[0m - Table RBMYBIS already exists.\u001b[0m\n",
      "[\u001b[34m2024-02-06T09:46:46.597+0800\u001b[0m] {\u001b[34m1887242903.py:\u001b[0m74} INFO\u001b[0m - LoadJob<project=personal-project-401309, location=US, id=cb2ccfed-6c70-4e54-9c09-787de1875058>\u001b[0m\n",
      "[\u001b[34m2024-02-06T09:46:46.598+0800\u001b[0m] {\u001b[34m1887242903.py:\u001b[0m75} INFO\u001b[0m - SUCCESS: The data has been loaded to Google BigQuery.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "selected_data = ['EXMAUS', 'RBMYBIS']\n",
    "df_list = fred_data_extraction(selected_data=selected_data)\n",
    "gsutil_uri_list = upload_to_bucket(storage_client=storage_client,\n",
    "                                   bucket_name=bucket_name,\n",
    "                                   df_list=df_list)\n",
    "upload_to_bigquery(client=bq_client,\n",
    "                   dataset_name=dataset_name,\n",
    "                   table_name=selected_data,\n",
    "                   job_config=job_config,\n",
    "                   gsutil_uri=gsutil_uri_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0108275a-8310-4e08-82f8-d9b3b581b9b5",
   "metadata": {},
   "source": [
    "### World Total Export Value from Inernational Monetary Fund (IMF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998e242b-780e-420c-bfcd-5d9777a539a0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Functions for IMF ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7b1f83d6-3774-4279-8d8e-27ced730400e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def dots(country, counterparts, start, end, freq='A', form=\"wide\"):\n",
    "    \n",
    "    \"\"\"\n",
    "    Highly flexible function to return time series trade data between countries from the IMF Direction of Trade (DOTS) Database.\n",
    "    The function sends a get request to the IMF JSON RESTful API. \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    country : str (required)\n",
    "        Country code for home country. \n",
    "        Use searches.country_codes() for a list of codes.\n",
    "        Use searches.country_search(\"keyword\") to search countries\n",
    "    counterparts : str or list (required)\n",
    "        Country code(s) for the counterpart country (or countries)\n",
    "        Use searches.country_codes() for a list of codes.\n",
    "        Use searches.country_search(\"keyword\") to search countries\n",
    "    start: int or float (reqiured)\n",
    "        Start date of the series. \n",
    "        Years may be entered as int dtype, such as 1980\n",
    "        Month start dates, such as 1980.02 (float) are acceptable.\n",
    "        Numbers such as 1980.00 and 1980.14 are unacceptable.\n",
    "    end: int or float (required)\n",
    "        Same as start date but end of the series.\n",
    "    freq: str (optional, default='A')\n",
    "        Frequency of the time series (intervals)\n",
    "        Default: 'A' - annual\n",
    "        Alternatives: 'M' - monthly\n",
    "        Note, freq \"A\" will override start dates entered as months, such as 1980.02\n",
    "        In this case, start will be rounded down to the nearest whole year\n",
    "        And end will be rounded up to the nearest whole year\n",
    "    form: str (optional, default='A')\n",
    "        If multiple counterparts, should the returned data be wide-form or long-form?\n",
    "        Default: 'wide' (MultiIndex)\n",
    "        Alternatives: 'long'\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    full_df : pandas.core.frame.DataFrame\n",
    "        DataFrame with trade statistics.\n",
    "        If multiple counterpart countries are selected and wide-form data is requested,\n",
    "        the resulting DataFrame will be multiIndexed/hierarchical\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> dots('US', 'CN', 1995, 2020)\n",
    "    Returns wide-form US-China annual data between 1995 and 2020.\n",
    "    \n",
    "    >>> dots('MX','W00', 2010, 2020, freq='M')\n",
    "    Returns wide-form Mexico-World monthly data between 2010 and 2020.\n",
    "    \n",
    "    >>> dots(\"GR\", [\"US\", \"AU\", \"DE\"], 1998, 2018)\n",
    "    Returns wide-form Greece annual data vs. the U.S., Australia and Germany \n",
    "    Between 1998 and 2018.\n",
    "        \n",
    "    >>> dots(\"XS25\", [\"JP\", \"KR\"], 2000.05, 2020.09, freq=\"M\", form=\"long\")\n",
    "    Returns long-form monthly data from Developing Asia vs. Japan and Korea\n",
    "    Between May 2005 and September 2009\n",
    "\n",
    "    \"\"\"\n",
    "    #validate input datatypes\n",
    "    assert isinstance(country, str), \"country must be a str\"\n",
    "    assert isinstance(counterparts, (str, list)), \"counterparts must be a str or list\"\n",
    "    # if isinstance(counterparts, list):\n",
    "    #     assert len(counterparts) > 1, \"counterparts must be a str or list of length 2 or more\"\n",
    "    #     assert country not in counterparts, \"country must not be in counterparts\"\n",
    "    # else:\n",
    "    #     assert country != counterparts, \"country and counterpart must not be the same\"\n",
    "    assert isinstance(start, (int,float)),\"start must be a number\"\n",
    "    assert isinstance(end, (int,float)), \"end must be a number\"\n",
    "    assert freq==\"M\" or freq==\"A\", \"frequency must be M or A\"\n",
    "    assert form in ['long', 'wide'], \"form must be long or wide\"\n",
    "    assert start > 1800 and start < 2200, \"start must be a reasonable date\"\n",
    "    assert end > 1800 and end < 2200, \"end must be a reasonable date\"\n",
    "    assert end >= start, \"end must be after start\"\n",
    "    \n",
    "    #transform mismatchedfrequency and start/end dates, if applicable\n",
    "    if freq==\"A\" and isinstance(start, float):\n",
    "        start = int(start)\n",
    "    if freq==\"A\" and isinstance(end, float):\n",
    "        end = int(end)+1\n",
    "    \n",
    "    #import libraries and define base URL for API\n",
    "    import requests, dateutil.parser\n",
    "    import pandas as pd\n",
    "    start_url = \"http://dataservices.imf.org/REST/SDMX_JSON.svc/\"\n",
    "    \n",
    "    #Specify all available series for trade (exports, imports and trade balance)\n",
    "    series = 'TBG_USD+TXG_FOB_USD+TMG_CIF_USD' \n",
    "    \n",
    "    #define subfunction to handle single requests\n",
    "    def retrieve(counterpart):\n",
    "        \n",
    "        request = f'{start_url}CompactData/DOT/{freq}.{country}.{series}.{counterpart}?startPeriod={start}&endPeriod={end}'\n",
    "\n",
    "        #Send the get request to the API\n",
    "        r = requests.get(request)\n",
    "        print(r)\n",
    "        \n",
    "        #assert the response was 200 (OK)\n",
    "        assert r.status_code==200, \"Error - HTTP Request unsuccessful. Please try again.\"\n",
    "            \n",
    "        #convert the data to subscriptable json \n",
    "        data_json = r.json()\n",
    "        \n",
    "        try:\n",
    "            #extract exports, imports and trade balance portions of the JSON (they are lists)\n",
    "            exports = data_json['CompactData']['DataSet']['Series'][0]['Obs']\n",
    "            imports = data_json['CompactData']['DataSet']['Series'][1]['Obs']\n",
    "            tbal = data_json['CompactData']['DataSet']['Series'][2]['Obs']\n",
    "            \n",
    "        #if series is not found, throw an error.    \n",
    "        except KeyError:\n",
    "            raise AssertionError(\"One or more series not found. Please try again.\")\n",
    "\n",
    "        #Make sure all series are the same length\n",
    "        assert len(exports)==len(imports), \"Error - data not available. Try a different time period or frequency.\"\n",
    "    \n",
    "        #Parse time periods and values for each series\n",
    "        periods = [dateutil.parser.parse(obs['@TIME_PERIOD']) for obs in exports]\n",
    "        values_exports = [float(obs['@OBS_VALUE']) for obs in exports]\n",
    "        values_imports = [float(obs['@OBS_VALUE']) for obs in imports]\n",
    "        values_tbal = [float(obs['@OBS_VALUE']) for obs in tbal]\n",
    "  \n",
    "        #Convert to a pandas dataframe\n",
    "        compile_df = pd.DataFrame({'Period':periods,\n",
    "                                'Exports':values_exports, \n",
    "                                'Imports':values_imports, \n",
    "                                'Trade Balance':values_tbal})\n",
    "        \n",
    "        #Inlucde a column for two-way trade (exports + imports)\n",
    "        compile_df['Twoway Trade']=compile_df['Exports']+compile_df['Imports']\n",
    "    \n",
    "        #Return the dataframe\n",
    "        return compile_df\n",
    "    \n",
    "    #subfunction to format date correctly based on user input frequency\n",
    "    def format_date(full_df):\n",
    "        if freq==\"A\":\n",
    "            full_df['Period'] = full_df['Period'].apply('{:%Y}'.format)\n",
    "        else:\n",
    "            full_df['Period'] = full_df['Period'].apply('{:%Y-%m}'.format)  \n",
    "        return full_df\n",
    "    \n",
    "    #if counterparts is a list of countries, send a request for each country\n",
    "    #append it to a master dataframe in long form\n",
    "    #pivot to wide and return\n",
    "    if isinstance(counterparts, list):\n",
    "        \n",
    "        full_df = pd.DataFrame()\n",
    "        for counterpart in counterparts:\n",
    "            retrieved = retrieve(counterpart)\n",
    "            retrieved.insert(1,\"Counterpart\",counterpart)\n",
    "            full_df = full_df.append(retrieved) #if long-form data requested, stop here\n",
    "        \n",
    "        #format dates correctly based on the user-specified frequency\n",
    "        full_df = format_date(full_df)\n",
    "        \n",
    "        if(form==\"wide\"): #otherwise, pivot to wide form data\n",
    "            full_df.insert(1,\"Country\",country)\n",
    "            full_df = full_df.pivot(index=\"Period\",\n",
    "                    columns='Counterpart', \n",
    "                    values=['Exports', 'Imports', 'Trade Balance', 'Twoway Trade'])\n",
    "            full_df.insert(0,\"Country\",country)\n",
    "        else:\n",
    "            full_df.insert(1,\"Country\",country) #if long-form data, insert country at position 1\n",
    "        \n",
    "    #if counterparts is a single country, create columns for country, counterpart\n",
    "    #and return the result of that single request\n",
    "    else:\n",
    "        full_df = retrieve(counterparts)\n",
    "        full_df.insert(1,'Country',country)\n",
    "        full_df.insert(2,'Counterpart',counterparts)\n",
    "        \n",
    "        #format dates correctly based on the user-specified frequency\n",
    "        full_df = format_date(full_df)\n",
    "        \n",
    "    return full_df\n",
    "\n",
    "def imf_extraction(countries, \n",
    "                   counterparts, \n",
    "                   start_year, \n",
    "                   end_year, \n",
    "                   frequency,):\n",
    "    \n",
    "    df_list = []\n",
    "    try:\n",
    "        for i in range(min(len(countries), len(counterparts))):\n",
    "            country = countries[i]\n",
    "            counterpart = counterparts[i]\n",
    "            logging.info(f\"Extracting {country}-{counterpart} data in progress ...\")\n",
    "            df = dots(country, counterpart, start_year, end_year, frequency)\n",
    "            logging.info(f\"SUCCESS: {country}-{counterpart} data has been extracted.\")\n",
    "            df.name = countries[i] + \"-\" + counterparts[i]\n",
    "            df_list.append(df)\n",
    "            return df_list\n",
    "    except Exception as e:\n",
    "            logging.error(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fbe243a-553c-4b0f-9e40-b6700b727f0d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Test Run for IMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7c10fd55-7e8f-49c0-b447-8c0960b56a2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "countries = ['W00']\n",
    "counterparts = ['W00']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "979cfbc6-6fb0-4590-8305-2fc345a4afc5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\u001b[34m2024-02-06T10:06:22.773+0800\u001b[0m] {\u001b[34m735304471.py:\u001b[0m190} INFO\u001b[0m - Extracting W00-W00 data in progress ...\u001b[0m\n",
      "<Response [200]>\n",
      "[\u001b[34m2024-02-06T10:06:23.371+0800\u001b[0m] {\u001b[34m735304471.py:\u001b[0m192} INFO\u001b[0m - SUCCESS: W00-W00 data has been extracted.\u001b[0m\n",
      "[\u001b[34m2024-02-06T10:06:23.568+0800\u001b[0m] {\u001b[34m1887242903.py:\u001b[0m21} INFO\u001b[0m - Bucket - trial-malaysia-international-trade is found.\u001b[0m\n",
      "[\u001b[34m2024-02-06T10:06:23.568+0800\u001b[0m] {\u001b[34m1887242903.py:\u001b[0m31} INFO\u001b[0m - Uploading data to Google Storage Bucket in progress ...\u001b[0m\n",
      "[\u001b[34m2024-02-06T10:06:23.927+0800\u001b[0m] {\u001b[34m1887242903.py:\u001b[0m33} INFO\u001b[0m - SUCCESS: <Blob: trial-malaysia-international-trade, W00-W00, 1707185182199680> has successfully uploaded to <Bucket: trial-malaysia-international-trade>.\u001b[0m\n",
      "[\u001b[34m2024-02-06T10:06:24.369+0800\u001b[0m] {\u001b[34m1887242903.py:\u001b[0m49} INFO\u001b[0m - Dataset trial_area already exists.\u001b[0m\n",
      "[\u001b[34m2024-02-06T10:06:24.751+0800\u001b[0m] {\u001b[34m1887242903.py:\u001b[0m61} INFO\u001b[0m - Table world_export already exists.\u001b[0m\n",
      "[\u001b[34m2024-02-06T10:06:27.698+0800\u001b[0m] {\u001b[34m1887242903.py:\u001b[0m74} INFO\u001b[0m - LoadJob<project=personal-project-401309, location=US, id=414749cc-2244-466b-8c79-5de63f68ba2f>\u001b[0m\n",
      "[\u001b[34m2024-02-06T10:06:27.699+0800\u001b[0m] {\u001b[34m1887242903.py:\u001b[0m75} INFO\u001b[0m - SUCCESS: The data has been loaded to Google BigQuery.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "df_list = imf_extraction(countries=countries,\n",
    "                         counterparts=counterparts,\n",
    "                         start_year=2000,\n",
    "                         end_year=2023,\n",
    "                         frequency=\"M\",)\n",
    "gsutil_uri_list = upload_to_bucket(storage_client=storage_client,\n",
    "                                   bucket_name=bucket_name,\n",
    "                                   df_list=df_list)\n",
    "upload_to_bigquery(client=bq_client,\n",
    "                   dataset_name=dataset_name,\n",
    "                   table_name=['world_export'],\n",
    "                   job_config=job_config,\n",
    "                   gsutil_uri=gsutil_uri_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a5cee35-4e27-4e33-9c09-f7d2c7db40be",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# To merge df1 and df2\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m final_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mmerge(\u001b[43mdf1\u001b[49m, df2)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# New column for MY export in USD currency and remove the initial export value\u001b[39;00m\n\u001b[0;32m      5\u001b[0m final_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExport\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m final_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexport\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m/\u001b[39mfinal_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mER\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df1' is not defined"
     ]
    }
   ],
   "source": [
    "# To merge df1 and df2\n",
    "final_df = pd.merge(df1, df2)\n",
    "\n",
    "# New column for MY export in USD currency and remove the initial export value\n",
    "final_df['Export'] = final_df['export']/final_df['ER']\n",
    "final_df = final_df.drop('export', axis = 1)\n",
    "\n",
    "# To assign units to the dataset\n",
    "final_df['Pop'] = final_df['Pop']/10**6 # in millions\n",
    "final_df['GDP'] = final_df['GDP']/10**9 # in billions\n",
    "final_df['Export'] = final_df['Export']/10**9 # in billions\n",
    "\n",
    "display(final_df.head())\n",
    "final_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f141e50-6821-4c7f-8c13-f43ce930db01",
   "metadata": {},
   "source": [
    "## Check Stationarity of Data\n",
    "## Unit Root Test - Augmented Dickey-Fuller (ADF) test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd4d100-bff9-464f-a024-03ad59f36566",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(nrows=3, ncols=2, figsize=(15, 9))\n",
    "\n",
    "def visualize_adfuller_results(series, title, ax):\n",
    "    result = adfuller(series)\n",
    "    significance_level = 0.05\n",
    "    adf_stat = result[0]\n",
    "    p_val = result[1]\n",
    "    crit_val_1 = result[4]['1%']\n",
    "    crit_val_5 = result[4]['5%']\n",
    "    crit_val_10 = result[4]['10%']\n",
    "\n",
    "    if (p_val < significance_level) & ((adf_stat < crit_val_1)):\n",
    "        linecolor = 'forestgreen' \n",
    "    elif (p_val < significance_level) & (adf_stat < crit_val_5):\n",
    "        linecolor = 'orange'\n",
    "    elif (p_val < significance_level) & (adf_stat < crit_val_10):\n",
    "        linecolor = 'red'\n",
    "    else:\n",
    "        linecolor = 'purple'\n",
    "    sns.lineplot(x=final_df['year'], y=series, ax=ax, color=linecolor)\n",
    "    ax.set_title(f'ADF Statistic {adf_stat:0.3f}, p-value: {p_val:0.3f}\\nCritical Values 1%: {crit_val_1:0.3f}, 5%: {crit_val_5:0.3f}, 10%: {crit_val_10:0.3f}', fontsize=14)\n",
    "    ax.set_ylabel(ylabel=title, fontsize=14)\n",
    "\n",
    "visualize_adfuller_results(final_df['ER'].values, 'Exchange Rate', ax[0, 0])\n",
    "visualize_adfuller_results(final_df['REER'].values, 'Real Effective Exchange Rate', ax[1, 0])\n",
    "visualize_adfuller_results(final_df['Pop'].values, 'Malaysia Population', ax[0, 1])\n",
    "visualize_adfuller_results(final_df['GDP'].values, 'Gross Domestic Produc', ax[1, 1])\n",
    "visualize_adfuller_results(final_df['Export'].values, 'Total Export of Malaysia', ax[2, 0])\n",
    "\n",
    "f.delaxes(ax[2, 1])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27612b6e-7ccd-42a4-ac17-cddb7e744790",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# First Order Differencing\n",
    "ts_diff = np.diff(final_df['Export'])\n",
    "final_df['Export1'] = np.append([0], ts_diff)\n",
    "\n",
    "ts_diff = np.diff(final_df['ER'])\n",
    "final_df['ER'] = np.append([0], ts_diff)\n",
    "\n",
    "ts_diff = np.diff(final_df['REER'])\n",
    "final_df['REER'] = np.append([0], ts_diff)\n",
    "\n",
    "ts_diff = np.diff(final_df['GDP'])\n",
    "final_df['GDP'] = np.append([0], ts_diff)\n",
    "\n",
    "# Reconduct the ADF test to check for the stationarity of data\n",
    "f, ax = plt.subplots(nrows=2, ncols=2, figsize=(17, 13.5))\n",
    "visualize_adfuller_results(final_df['ER'].values, 'Differenced (1. Order) \\n Exchange Rate', ax[0, 0])\n",
    "visualize_adfuller_results(final_df['REER'].values, 'Differenced (1. Order) \\n Real Effective Exchange Rate', ax[1, 0])\n",
    "visualize_adfuller_results(final_df['GDP'].values, ' Differenced (1. Order) \\nGross Domestic Produc', ax[1, 1])\n",
    "visualize_adfuller_results(final_df['Export1'].values, ' Differenced (1. Order) \\nTotal Export of Malaysia', ax[0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c799c1ba-35f6-4ad3-9409-44c07eba3314",
   "metadata": {},
   "source": [
    "# Data Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f23c175-6050-49b6-be7d-f29bbd0bdf23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_corr = final_df.copy()\n",
    "\n",
    "# Create a copy of the DataFrame with the target variable dropped\n",
    "df_corr = df_corr.drop(['Export', 'year'] , axis=1)\n",
    "\n",
    "# Create a correlation matrix\n",
    "corr_matrix = df_corr.corr()\n",
    "\n",
    "# Create a mask to hide the upper triangular portion of the heatmap\n",
    "mask = np.zeros_like(corr_matrix, dtype=np.bool_)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "# Create a figure and axis object\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "\n",
    "# Create a heatmap of the correlation matrix using seaborn\n",
    "sns.heatmap(corr_matrix, cmap='viridis', annot=True, vmin=-1, vmax=1, mask=mask, fmt=\".2f\", square=True, ax=ax)\n",
    "\n",
    "# Set the title of the plot\n",
    "ax.set_title('Correlation Matrix')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d214723-9ccd-475c-a298-b354d9a652c5",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "## Data Splitting into Training and Testing Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70d8c28-4e9e-44b3-9c6f-795dfe415afb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(final_df))\n",
    "test_size = len(final_df) - train_size \n",
    "\n",
    "print(train_size, test_size)\n",
    "\n",
    "univariate_df = final_df[['year', 'Export1']].copy()\n",
    "\n",
    "train_data = univariate_df.iloc[:train_size, :]\n",
    "\n",
    "x_train, y_train = pd.DataFrame(univariate_df.iloc[:train_size, 0]), pd.DataFrame(univariate_df.iloc[:train_size, 1])\n",
    "x_test, y_test = pd.DataFrame(univariate_df.iloc[train_size:, 0]), pd.DataFrame(univariate_df.iloc[train_size:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55814b6b-77ee-4966-8395-6df755d82105",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#plt.figure(figsize=(10,6))\n",
    "plt.grid(True)\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Total Export Value')\n",
    "plt.plot(x_train, y_train, 'green', label='Train data')\n",
    "plt.plot(x_test, y_test, 'blue', label='Test data')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4902341c-4391-4ef3-b1be-d8b48197b40c",
   "metadata": {},
   "source": [
    "## Autoregressive Integrated Moving Average (ARIMA) Model\n",
    "\n",
    "### Identification of the best performance ARIMA model via auto-arima\n",
    "Auto ARIM helps to automatically identify the optimal order for an ARIMA model.\n",
    "The auto_arima function seeks to identify the most optimal parameters for an ARIMA model, and returns a fitted ARIMA model. This function is based on the commonly-used R function, forecast::auto.arima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdefa3ef-728c-4003-88a5-111fd74f57cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_autoARIMA = auto_arima(y_train, start_p=0, start_q=0, \n",
    "                             test ='adf',       # use adftest to find optimal 'd'\n",
    "                              max_p=3, max_q=3, # maximum p and q\n",
    "                              m=1,              # frequency of series\n",
    "                              d=None,           # let model determine 'd'\n",
    "                              seasonal=False,   # No Seasonality\n",
    "                              start_P=0, \n",
    "                              D=0, \n",
    "                              trace=True,\n",
    "                              error_action='ignore',  \n",
    "                              suppress_warnings=True, \n",
    "                              stepwise=True)\n",
    "\n",
    "print(model_autoARIMA.summary())\n",
    "model_autoARIMA.plot_diagnostics(figsize=(15,8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f966a72f-e032-405f-909a-7af7056db179",
   "metadata": {},
   "source": [
    "### Fit the data into the ARIMA (3, 2, 0) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a4903c-3683-4411-b705-3d413d18d03b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fit model\n",
    "model = ARIMA(y_train, order=(3,2,0))\n",
    "\n",
    "model_fit = model.fit()\n",
    "print(model_fit.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb423ed-d426-4620-bf68-4795392840f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Forecast the results using the test data\n",
    "fc = model_fit.forecast(5, alpha=0.05)  # 95% conf\n",
    "print(fc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704f65fb-533b-4fce-8a5c-a9eb1789b4a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Make as pandas series\n",
    "fc_series = pd.Series(fc)\n",
    "\n",
    "# PLot graph to view the predicted and actual total export value\n",
    "plt.figure(figsize=(10,5), dpi=100)\n",
    "plt.plot(x_train, y_train, label='training data')\n",
    "plt.plot(x_test, y_test, color = 'blue', label='Actual Stock Price')\n",
    "plt.plot(x_test, fc_series, color = 'orange',label='Predicted Stock Price')\n",
    "plt.title('Total Export Value of Malaysia')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Total Export Value (in billions USD)')\n",
    "plt.legend(loc='upper left', fontsize=8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a3df95-bf1f-414a-81f6-284d3e5cee0d",
   "metadata": {},
   "source": [
    "### Peformance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7535eeab-de54-4df7-a56e-24e985377fbb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rmse = math.sqrt(mean_squared_error(y_test, fc))\n",
    "print('RMSE: '+str(rmse))\n",
    "mape = np.mean(np.abs(y_test.iloc[:,0] - fc)/np.abs(y_test.iloc[:,0]))\n",
    "print('MAPE: '+str(mape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d9dc8e-aca1-4649-a474-1f7469ec191c",
   "metadata": {},
   "source": [
    "## Lazy Classifier\n",
    "A quick estimation on the performance of other regressor models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96dee802-cfce-40a9-b871-5dc8040dbf73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# copy df\n",
    "df_lc= final_df.copy()\n",
    "\n",
    "# separate features and target variable\n",
    "data = df_lc.drop(['Export1', 'Export'] , axis=1)\n",
    "target = df_lc['Export1']\n",
    "\n",
    "\n",
    "X, y = shuffle(data, target, random_state=13)\n",
    "offset = int(X.shape[0] * 0.5)\n",
    "\n",
    "X_train, y_train = X[:offset], y[:offset]\n",
    "X_test, y_test = X[offset:], y[offset:]\n",
    "\n",
    "\n",
    "# initialize LazyClassifier\n",
    "reg = LazyRegressor(verbose=0, ignore_warnings=False, custom_metric=None)\n",
    "\n",
    "# fit and predict using multiple machine learning models\n",
    "models, predictions = reg.fit(X_train, X_test, y_train, y_test)\n",
    "\n",
    "print(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748d1bc1-2dae-4fd5-b0f1-238544564f37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
